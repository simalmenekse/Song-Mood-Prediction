{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compound-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general packages\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import ntpath\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import tkinter packages\n",
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "from tkinter import messagebox\n",
    "from tkinter.ttk import Progressbar\n",
    "from tkinter.ttk import Button\n",
    "\n",
    "# import music analysis packages\n",
    "import librosa\n",
    "import pyAudioAnalysis as pyaudio\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import MidTermFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continuous-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "persistent-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the directory of the mp3 files using tkinter\n",
    "tkobj = tkinter.Tk()\n",
    "tkobj.withdraw()\n",
    "tkobj.directory = filedialog.askdirectory(title = \"Select the folder containing the MP3 files.\")\n",
    "path = tkobj.directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer as soon as a directory is chosen\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for progress button\n",
    "# progress = Progressbar(tkobj, length = 100, mode = 'determinate')\n",
    "# progress.pack()\n",
    "# Button(tkobj, text = 'Generate Playlists').pack()\n",
    "# tkobj.mainloop()\n",
    "# progress['value'] = 20\n",
    "# tkobj.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-reynolds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file 1 of 27: D:/Temporary Music AML\\Castaways (Indie Pop).mp3\n",
      "Error: file not found or other I/O error. (DECODING FAILED)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shari\\Desktop\\Uni\\Applied Machine Learning\\Project\\AML GIT\\AppliedMachineLearning\\RAW Project\\application\\moody_playlist.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m short_term_step \u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m root, dirs, files \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(path, topdown \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# extract pyAudioAnalysis features from the root folder\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     pyaudio_feat, song_files, feat_names \u001b[39m=\u001b[39m MidTermFeatures\u001b[39m.\u001b[39;49mdirectory_feature_extraction(root, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                                                                                         mid_term_window, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                                                                                         mid_term_step, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                                                                         short_term_window, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                                                                         short_term_step,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                                                                         \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# convert pyAudioAnalysis features into dictionary format\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shari/Desktop/Uni/Applied%20Machine%20Learning/Project/AML%20GIT/AppliedMachineLearning/RAW%20Project/application/moody_playlist.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyAudioAnalysis\\MidTermFeatures.py:194\u001b[0m, in \u001b[0;36mdirectory_feature_extraction\u001b[1;34m(folder_path, mid_window, mid_step, short_window, short_step, compute_beat)\u001b[0m\n\u001b[0;32m    191\u001b[0m     beat, beat_conf \u001b[39m=\u001b[39m beat_extraction(short_features, short_step)\n\u001b[0;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     mid_features, _, mid_feature_names \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 194\u001b[0m         mid_feature_extraction(signal, sampling_rate,\n\u001b[0;32m    195\u001b[0m                                \u001b[39mround\u001b[39;49m(mid_window \u001b[39m*\u001b[39;49m sampling_rate),\n\u001b[0;32m    196\u001b[0m                                \u001b[39mround\u001b[39;49m(mid_step \u001b[39m*\u001b[39;49m sampling_rate),\n\u001b[0;32m    197\u001b[0m                                \u001b[39mround\u001b[39;49m(sampling_rate \u001b[39m*\u001b[39;49m short_window),\n\u001b[0;32m    198\u001b[0m                                \u001b[39mround\u001b[39;49m(sampling_rate \u001b[39m*\u001b[39;49m short_step))\n\u001b[0;32m    200\u001b[0m mid_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(mid_features)\n\u001b[0;32m    201\u001b[0m mid_features \u001b[39m=\u001b[39m mid_features\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyAudioAnalysis\\MidTermFeatures.py:94\u001b[0m, in \u001b[0;36mmid_feature_extraction\u001b[1;34m(signal, sampling_rate, mid_window, mid_step, short_window, short_step)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmid_feature_extraction\u001b[39m(signal, sampling_rate, mid_window, mid_step,\n\u001b[0;32m     88\u001b[0m                            short_window, short_step):\n\u001b[0;32m     89\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m    Mid-term feature extraction\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     short_features, short_feature_names \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> 94\u001b[0m         ShortTermFeatures\u001b[39m.\u001b[39;49mfeature_extraction(signal, sampling_rate,\n\u001b[0;32m     95\u001b[0m                                              short_window, short_step)\n\u001b[0;32m     97\u001b[0m     n_stats \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     98\u001b[0m     n_feats \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(short_features)\n",
      "File \u001b[1;32mc:\\Users\\shari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyAudioAnalysis\\ShortTermFeatures.py:570\u001b[0m, in \u001b[0;36mfeature_extraction\u001b[1;34m(signal, sampling_rate, window, step, deltas)\u001b[0m\n\u001b[0;32m    567\u001b[0m signal \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdouble(signal)\n\u001b[0;32m    568\u001b[0m signal \u001b[39m=\u001b[39m signal \u001b[39m/\u001b[39m (\u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m15\u001b[39m)\n\u001b[1;32m--> 570\u001b[0m signal \u001b[39m=\u001b[39m dc_normalize(signal)\n\u001b[0;32m    572\u001b[0m number_of_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(signal)  \u001b[39m# total number of samples\u001b[39;00m\n\u001b[0;32m    573\u001b[0m current_position \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyAudioAnalysis\\ShortTermFeatures.py:18\u001b[0m, in \u001b[0;36mdc_normalize\u001b[1;34m(sig_array)\u001b[0m\n\u001b[0;32m     16\u001b[0m sig_array_norm \u001b[39m=\u001b[39m sig_array\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     17\u001b[0m sig_array_norm \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m sig_array_norm\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> 18\u001b[0m sig_array_norm \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39;49m(sig_array_norm)\u001b[39m.\u001b[39;49mmax() \u001b[39m+\u001b[39m \u001b[39m1e-10\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m sig_array_norm\n",
      "File \u001b[1;32mc:\\Users\\shari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_amax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m           initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[39mNone\u001b[39;49;00m, out, keepdims, initial, where)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# extract raw features\n",
    "m3u_paths = {}\n",
    "librosa_features = {}\n",
    "pyaudio_features = {}\n",
    "feat_names = None\n",
    "\n",
    "# pyAudioAnalysis feature extraction variables\n",
    "mid_term_window = 1\n",
    "mid_term_step = 1\n",
    "short_term_window = 0.05\n",
    "short_term_step = 0.05\n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown = False):\n",
    "    # extract pyAudioAnalysis features from the root folder\n",
    "    pyaudio_feat, song_files, feat_names = MidTermFeatures.directory_feature_extraction(root, \n",
    "                                                                                        mid_term_window, \n",
    "                                                                                        mid_term_step, \n",
    "                                                                                        short_term_window, \n",
    "                                                                                        short_term_step,\n",
    "                                                                                        False)\n",
    "    # convert pyAudioAnalysis features into dictionary format\n",
    "    counter = 0\n",
    "    for s in song_files:\n",
    "        s_dict_name = ntpath.basename(s)\n",
    "        if pyaudio_feat.ndim == 1:\n",
    "            pyaudio_features[s_dict_name] = pyaudio_feat\n",
    "        else:\n",
    "            pyaudio_features[s_dict_name] = pyaudio_feat[counter]\n",
    "        counter += 1\n",
    "    \n",
    "    # extract pyAudioAnalysis features from subfolders\n",
    "    for folder in dirs:\n",
    "        folder_path = os.path.join(root, folder)\n",
    "        pyaudio_feat, song_files, feat_names = MidTermFeatures.directory_feature_extraction(folder_path, \n",
    "                                                                                            mid_term_window, \n",
    "                                                                                            mid_term_step, \n",
    "                                                                                            short_term_window, \n",
    "                                                                                            short_term_step,\n",
    "                                                                                            False)\n",
    "        # convert pyAudioAnalysis features into dictionary format\n",
    "        counter = 0\n",
    "        for s in song_files:\n",
    "            s_dict_name = ntpath.basename(s)\n",
    "            if pyaudio_feat.ndim == 1:\n",
    "                pyaudio_features[s_dict_name] = pyaudio_feat\n",
    "            else:\n",
    "                pyaudio_features[s_dict_name] = pyaudio_feat[counter]\n",
    "            counter += 1\n",
    "    \n",
    "    \n",
    "    # extract librosa features\n",
    "    for song in files:\n",
    "        song_path = os.path.join(root, song)\n",
    "        if song_path.endswith(\".mp3\"):\n",
    "            # get the tempo of the song\n",
    "            waveform, samp_rate = librosa.load(song_path)\n",
    "            tempo, beat_frames = librosa.beat.beat_track(waveform, samp_rate)\n",
    "\n",
    "            # get the chroma number of the song\n",
    "            beat_times = librosa.frames_to_time(beat_frames, samp_rate)\n",
    "            y_harmonic, y_percussive = librosa.effects.hpss(waveform)\n",
    "            chromagram = librosa.feature.chroma_cqt(y_harmonic, samp_rate)\n",
    "            beat_chroma = librosa.util.sync(chromagram, beat_frames, aggregate = np.median)\n",
    "            \n",
    "            # make beat chroma into a DataFrame and calculate the diff for a single float\n",
    "            chroma_df = pd.DataFrame(beat_chroma)\n",
    "            diff_values = chroma_df.diff()\n",
    "            diff_mean = diff_values.mean(axis = 0, skipna = True)\n",
    "            chroma_num = sum(diff_mean) / len(diff_mean)\n",
    "            \n",
    "            # save librosa features and the path to each song\n",
    "            librosa_features[song] = [tempo, chroma_num]\n",
    "            m3u_paths[song] = song_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show an error message if the feature extraction failed\n",
    "if len(pyaudio_features) != len(librosa_features):\n",
    "    messagebox.showinfo(\"Error\", \"Import of MP3 files failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep desired features from pyAudioAnalysis for stacked classifier\n",
    "new_pyaudio_features = {}\n",
    "selected_feats = ['zcr_mean', 'zcr_std', 'energy_mean', 'energy_entropy_mean', 'spectral_centroid_mean', \n",
    "                  'spectral_spread_mean', 'spectral_entropy_mean', 'mfcc_2_mean', 'mfcc_5_mean', 'mfcc_6_mean',\n",
    "                  'spectral_centroid_std', 'spectral_entropy_std', 'spectral_spread_std', 'chroma_7_std',\n",
    "                  'delta chroma_2_std', 'delta chroma_3_std', 'delta chroma_9_std', 'delta chroma_std_std',\n",
    "                  'delta energy_std', 'delta mfcc_1_std', 'delta mfcc_3_std', 'delta mfcc_13_std',\n",
    "                  'delta spectral_centroid_std', 'delta spectral_entropy_std', 'delta spectral_flux_std',\n",
    "                  'delta spectral_spread_std']\n",
    "for key, value in pyaudio_features.items():\n",
    "    index_list = [feat_names.index(feat) for feat in selected_feats]\n",
    "    extracted_feats = [value[i] for i in index_list]\n",
    "    new_pyaudio_features[key] = extracted_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose and merge the engineered features into one feature DataFrame\n",
    "librosa_df = pd.DataFrame(librosa_features)\n",
    "librosa_df = librosa_df.transpose()\n",
    "eng_pyaudio_df = pd.DataFrame(new_pyaudio_features)\n",
    "eng_pyaudio_df = eng_pyaudio_df.transpose()\n",
    "eng_features = librosa_df.merge(eng_pyaudio_df, right_index = True, left_index = True)\n",
    "eng_features.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate neural network features\n",
    "pyaudio_df = pd.DataFrame(pyaudio_features)\n",
    "pyaudio_df = pyaudio_df.transpose()\n",
    "nn_features = pyaudio_df\n",
    "nn_features.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stacking classifier model and predict using the engineered features\n",
    "loaded_stacker = pickle.load(open('mood_stacking_model.sav', 'rb'))\n",
    "stacker_predictions = loaded_stacker.predict(eng_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mlp model and predict using the neural network features\n",
    "loaded_mlp = pickle.load(open('mood_mlp_model.sav', 'rb'))\n",
    "mlp_predictions = loaded_mlp.predict(nn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine predictions from both models for a final predictions list\n",
    "final_preds = []\n",
    "\n",
    "for i in range(len(stacker_predictions)):\n",
    "    stack_pred = stacker_predictions[i]\n",
    "    mlp_pred = mlp_predictions[i]\n",
    "    \n",
    "    if stack_pred != mlp_pred:\n",
    "        # prediction was different between stacking and mlp models\n",
    "        if stack_pred == 1 or stack_pred == 3:\n",
    "            # prefer stacking classifier for moods 1 and 3\n",
    "            final_preds.append(stack_pred)\n",
    "        else:\n",
    "            final_preds.append(mlp_pred)\n",
    "    else:\n",
    "        # predictions are the same, just append mlp's prediction\n",
    "        final_preds.append(mlp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate song lists based on predicted moods\n",
    "# 1 epic, 2 lighthearted, 3 energetic, 4 calm, 5 chill, 6 miscellaneous\n",
    "epic = []\n",
    "lighthearted = []\n",
    "energetic = []\n",
    "calm = []\n",
    "chill = []\n",
    "miscellaneous = []\n",
    "\n",
    "# ordering of indices are the same as engineered features due to prior sorting\n",
    "for i in range(len(final_preds)):\n",
    "    song = eng_features.index[i]\n",
    "    pred = final_preds[i]\n",
    "    if pred == 1:\n",
    "        epic.append(song)\n",
    "    if pred == 2:\n",
    "        lighthearted.append(song)\n",
    "    if pred == 3:\n",
    "        energetic.append(song)\n",
    "    if pred == 4:\n",
    "        calm.append(song)\n",
    "    if pred == 5:\n",
    "        chill.append(song)\n",
    "    if pred == 6:\n",
    "        miscellaneous.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the relative paths of each song for m3u file format\n",
    "m3u_paths_relative = {}\n",
    "for key, value in m3u_paths.items():\n",
    "    root_length = len(path) + 1\n",
    "    m3u_paths_relative[key] = value[root_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate m3u files based on song lists\n",
    "def write_playlist(path, filename, mood_list):\n",
    "    full_path = path + \"/\" + filename + \".m3u\"\n",
    "    file = open(full_path, \"w\")\n",
    "    playlist = [m3u_paths_relative[song] + \"\\n\" for song in mood_list]\n",
    "    playlist.insert(0, \"#EXTM3U\\n\")\n",
    "    file.writelines(playlist)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the playlist files\n",
    "write_playlist(path, \"epic_playlist\", epic)\n",
    "write_playlist(path, \"lighthearted_playlist\", lighthearted)\n",
    "write_playlist(path, \"energetic_playlist\", energetic)\n",
    "write_playlist(path, \"calm_playlist\", calm)\n",
    "write_playlist(path, \"chill_playlist\", chill)\n",
    "write_playlist(path, \"miscellaneous_playlist\", miscellaneous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the timer when files are done writing\n",
    "end = time.time()\n",
    "time_elapsed = round((end - start) / 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the time elapsed in a message box\n",
    "message = \"Total Time Elapsed: \" + str(time_elapsed) + \" minutes\"\n",
    "messagebox.showinfo(\"Playlists have been successfully created.\", message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
